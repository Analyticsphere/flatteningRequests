---
title: "Generate Query from Source Table Schema & Merge w/ Manaul Query"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

#### 0. Load dependencies and define parameters

```{r, setup}
library(jsonlite)
library(tools)
library(reticulate)
library(bigrquery)

# Load helper functions
source("gcp_query_tools/connect_analytics_helper_functions.R")

########### DEFINE PARAMETERS ###########################
# project   <- "nih-nci-dceg-connect-dev"
# project   <- "nih-nci-dceg-connect-stg-5519"
project <- "nih-nci-dceg-connect-prod-6d04"
table   <- "Connect.participants" 
#########################################################

# Authenticate to BigQuery
bq_auth()
```

#### 1. Get schema from BigQuery as a JSON file (only format option)

```{r}
# Generate file names given table name
module        <- strsplit(table, split=".", fixed=TRUE)[[1]][2]
query_folder  <- "queryGenerators/FlatConnect_queries/"
module_folder <- paste0(query_folder, module, "/")

# Create temporary directory to store temp files in
tmp_folder <- paste0(module_folder,"tmp/")
if (!dir.exists(tmp_folder)) {dir.create(tmp_folder)}

# Name schema files
schema_json   <- paste0(tmp_folder,module,"-schema.json")
schema_csv    <- paste0(tmp_folder,module,"-schema.csv")

# Set default GCP project
set_default_gcp_project(project)

# Get the table schema from GCP as a JSON file named schema_json
get_gcp_table_schema(project, table, schema_json) 
```

#### 2. Convert schema from JSON to CSV

# Set up python
```{r}
# Set environment variables
# Variables with these properties will be filtered out. String cannot contain spaces.
schema_filter <- '{"name":["__key__","__error__","__has_error__","treeJSON","allEmails","allPhoneNo"],"type":["RECORD"]}'
py_func <- 'gcp_query_tools/json_schema_to_csv_b.py'
Sys.setenv(SCHEMA_CSV = schema_csv)
Sys.setenv(SCHEMA_JSON = schema_json)
Sys.setenv(SCHEMA_FILTER = schema_filter)
Sys.setenv(PY_FUNC = py_func)
system('python3 $PY_FUNC $SCHEMA_JSON $SCHEMA_CSV $SCHEMA_FILTER')
```



#### 3. Generate \*-variables.csv and \*-lists.json file from source table schema.

Here, we'll generate a new query from a BigQuery table schema, Our
queries require a query_generator.js, \*-variables.csv and a
\*-lists.json. This function generates filters the variables in the
schema based on whether they should be in the variables.csv file or the
\*-list.json file. All variables destined for the list file are assigned
a list of CIDs that are present in the data set for that variable,
across all rows.

```{r}
# Generate the query from the schema in the form of a set of 
# variables.csv and lists.json files.
out_json <- paste0(tmp_folder, module, "-lists-from-schema.json")
out_csv  <- paste0(tmp_folder, module, "-variables-from-schema.csv")

# Get variables and lists files from schema
filter_vars_from_schema(project, table, schema_csv, out_csv, out_json, 
                               output_file_type = "json")
```

#### 4. Combine json and csv files prior to generating query

Now that we have a new query in the form of variables.csv and lists.json
files, we need to combine the new json file with the old json file.

```{r}
# -lists.json file from parent folder
old_lists_json <- paste0(module_folder, module, "-lists.json")
old_lists_json

# name combined json file
union_lists_json <- paste0(tmp_folder, module, "-lists-union.json")
union_lists_json

# Get union of old_lists_json and out_json
union_json_str <- get_union_of_json_arrays(old_lists_json, out_json, 
                                           union_lists_json)

print(union_json_str)
```

We also need to combine the variables.csv file that was generated from
the schema with the old, manually generated variables.csv file. This
code will exclude any duplicates.

```{r}
# Get union of variable files
# -lists.json file from parent folder
old_vars_csv   <- paste0(module_folder, module, "-variables.csv")

# name combined csv file
union_vars_csv <- paste0(tmp_folder, module, "-vars-union.csv")
union_vars_csv

union_var_chr_array <- get_union_of_variable_csv(old_vars_csv, out_csv, 
                                                 union_vars_csv)
```

#### 5. Deal with exceptions

```{r}
# Define exceptions
exceptions <- c(178420302, 958239616, 353358909, 104430631)

# Get a list of variables with arrays containing "exception CID"
vars_w_exceptions <- get_vars_with_specified_vals(union_lists_json, 
                                                  exceptions, 
                                                  max_array_len = 2) 
vars_w_exceptions

# Remove list of variables with exceptions from the data set
new_lists_json    <- paste0(tmp_folder, module, "-lists-union-mod.json")
updated_variables <- remove_vars_from_json(union_lists_json, 
                                           vars_w_exceptions,
                                           output_filename = new_lists_json) 

# Add list of variables with exceptions to \*-variables\*.csv
new_vars_csv  <- paste0(tmp_folder, module, "-variables-union-mod.csv")

add_novel_vars_to_csv(vars_w_exceptions, union_vars_csv, 
                      out_file = new_vars_csv)
```

Get list of variables from csv file that are of type RECORD or mode
REPEATED in the schema. There shouldn't be any left at this point, but
just in case...

```{r}
#TODO This chunk needs to be tested.
vars_to_remove <- get_record_and_repeated_vars(project, table, schema_csv, 
                                              new_vars_csv)
vars_to_remove

# Remove these variables from the new csv file
if ( length(vars_to_remove ) != 0) {
  remove_vars_from_csv(new_vars_csv, vars_to_remove)
}
```

#### 6. Move modified lists.json and variables.csv files to parent directory

```{r}
# Move up-to-date files to parent directory
file.rename(new_vars_csv, old_vars_csv)
file.rename(new_lists_json, old_lists_json)
```

#### 7. Clean up intermediate files from working directory

```{r}
# Delete intermediate files from directory
unlink(tmp_folder, recursive=TRUE)
```

#### 8. Generate query using Warren Lu-style Query Generator

```{r}
working_dir     <- getwd()
query_generator <- paste0(module, "-query_generator.js")
Sys.setenv(MODULE_FOLDER = module_folder,
           QUERY_GENERATOR = query_generator,
           WORKNG_DIR = working_dir)

# Using shell commands, go to the module folder, run the query generator (node.js),
# and return to the working directory.
system("cd ${MODULE_FOLDER} && node ${QUERY_GENERATOR}; cd ${WORKING_DIR}")
```

#### 9. TODO: Update scheduled queries in GCP programatically



```{python}
from google.cloud import bigquery_datatransfer
from google.protobuf import field_mask_pb2
import json

# Load configuration file
query_config_file = r.module_folder + "query-config.json"
with open(query_config_file) as f:
  query_config = json.load(f)

print(query_config["resource_name"]["dev"])
print(query_config["resource_name"]["stg"])
print(query_config["resource_name"]["prod"])

#TODO: Loop through each tier

query_file = r.module_folder + query_config["query_name"] + "-prod.sql"
print("query_file: " + query_file)
with open(query_file) as file:
    query_str = file.read()
print("query_str: \n\n" + query_str[0:200])
    
query_string = """q""".replace("q", query_str)

service_account_name = "qa-qc-prod@nih-nci-dceg-connect-prod-6d04.iam.gserviceaccount.com"
#TODO: Put transfer configs in config file for each tier and write a function to get transfer_config_name from GCP.

transfer_config_name = "projects/155089172944/locations/us/transferConfigs/63a134bd-0000-2c48-8c16-883d24f91d30"
new_display_name = r.module_folder + query_config["query_name"]

transfer_client = bigquery_datatransfer.DataTransferServiceClient()

transfer_config = bigquery_datatransfer.TransferConfig(name=transfer_config_name)
transfer_config.display_name = new_display_name
transfer_config.params = {"query": query_string, "write_disposition": "WRITE_TRUNCATE"}

transfer_config = transfer_client.update_transfer_config(
    {
        "transfer_config": transfer_config,
        "update_mask": field_mask_pb2.FieldMask(paths=["display_name"]),
    }
)

print(f"Updated config: '{transfer_config.name}'")
print(f"New display name: '{transfer_config.display_name}'")
```
