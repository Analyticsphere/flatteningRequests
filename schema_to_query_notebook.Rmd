---
title: "Generate Query from Source Table Schema & Merge w/ Manaul Query"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

#### 0. Load dependencies and define parameters

```{r, setup}
library(jsonlite)
library(tools)
library(reticulate)
library(bigrquery)

# Load helper functions
source("gcp_query_tools/connect_analytics_helper_functions.R")

########### DEFINE PARAMETERS ###########################
# project   <- "nih-nci-dceg-connect-dev"
# project   <- "nih-nci-dceg-connect-stg-5519"
project <- "nih-nci-dceg-connect-prod-6d04"
table   <- "Connect.module4_v1" 
table_slection <- 1
table   <- switch(table_selection,
                  "biospecimen",  # 1
                  "bioSurvey_v1", # 2
                  "clinicalBioSurvey_v1", # 3
                  "menstrualSurvey_v1",   # 4
                  "module1_v1",  # 5
                  "module1_v2",  # 6
                  "module2_v1",  # 7
                  "module1_v2",  # 8
                  "module3_v1",  # 9
                  "module4_v1",  # 10
                  "participants" # 11
                  )
#########################################################

# Authenticate to BigQuery
bq_auth()
```

#### 1. Get schema from BigQuery as a JSON file (only format option)

```{r}
# Generate file names given table name
module        <- strsplit(table, split=".", fixed=TRUE)[[1]][2]
query_folder  <- "queryGenerators/FlatConnect_queries/"
module_folder <- paste0(query_folder, module, "/")

# Create temporary directory to store temp files in
tmp_folder <- paste0(module_folder,"tmp/")
if (!dir.exists(tmp_folder)) {dir.create(tmp_folder)}

# Name schema files
schema_json   <- paste0(tmp_folder,module,"-schema.json")
schema_csv    <- paste0(tmp_folder,module,"-schema.csv")

# Set default GCP project
set_default_gcp_project(project)

# Get the table schema from GCP as a JSON file named schema_json
get_gcp_table_schema(project, table, schema_json) 
```

#### 2. Convert schema from JSON to CSV

# Set up python
```{r}
# Set environment variables
# Variables with these properties will be filtered out. String cannot contain spaces.
schema_filter <- '{"name":["__key__","__error__","__has_error__","treeJSON","allEmails","allPhoneNo"],"type":["RECORD"]}'
py_func <- 'gcp_query_tools/json_schema_to_csv_b.py'
Sys.setenv(SCHEMA_CSV = schema_csv)
Sys.setenv(SCHEMA_JSON = schema_json)
Sys.setenv(SCHEMA_FILTER = schema_filter)
Sys.setenv(PY_FUNC = py_func)
system('python3 $PY_FUNC $SCHEMA_JSON $SCHEMA_CSV $SCHEMA_FILTER')
```



#### 3. Generate \*-variables.csv and \*-lists.json file from source table schema.

Here, we'll generate a new query from a BigQuery table schema, Our
queries require a query_generator.js, \*-variables.csv and a
\*-lists.json. This function generates filters the variables in the
schema based on whether they should be in the variables.csv file or the
\*-list.json file. All variables destined for the list file are assigned
a list of CIDs that are present in the data set for that variable,
across all rows.

```{r}
# Generate the query from the schema in the form of a set of 
# variables.csv and lists.json files.
out_json <- paste0(tmp_folder, module, "-lists-from-schema.json")
out_csv  <- paste0(tmp_folder, module, "-variables-from-schema.csv")

# Get variables and lists files from schema
filter_vars_from_schema(project, table, schema_csv, out_csv, out_json, 
                               output_file_type = "json")
```

#### 4. Combine json and csv files prior to generating query

Now that we have a new query in the form of variables.csv and lists.json
files, we need to combine the new json file with the old json file.

```{r}
# -lists.json file from parent folder
old_lists_json <- paste0(module_folder, module, "-lists.json")
old_lists_json

# name combined json file
union_lists_json <- paste0(tmp_folder, module, "-lists-union.json")
union_lists_json

# Get union of old_lists_json and out_json
union_json_str <- get_union_of_json_arrays(old_lists_json, out_json, 
                                           union_lists_json)

print(union_json_str)
```

We also need to combine the variables.csv file that was generated from
the schema with the old, manually generated variables.csv file. This
code will exclude any duplicates.

```{r}
# Get union of variable files
# -lists.json file from parent folder
old_vars_csv   <- paste0(module_folder, module, "-variables.csv")

# name combined csv file
union_vars_csv <- paste0(tmp_folder, module, "-vars-union.csv")
union_vars_csv

union_var_chr_array <- get_union_of_variable_csv(old_vars_csv, out_csv, 
                                                 union_vars_csv)
```

#### 5. Deal with exceptions

```{r}
# Define exceptions
exceptions <- c(178420302, 958239616, 353358909, 104430631)

# Get a list of variables with arrays containing "exception CID"
vars_w_exceptions <- get_vars_with_specified_vals(union_lists_json, 
                                                  exceptions, 
                                                  max_array_len = 2) 
vars_w_exceptions

# Remove list of variables with exceptions from the data set
new_lists_json    <- paste0(tmp_folder, module, "-lists-union-mod.json")
updated_variables <- remove_vars_from_json(union_lists_json, 
                                           vars_w_exceptions,
                                           output_filename = new_lists_json) 

# Add list of variables with exceptions to \*-variables\*.csv
new_vars_csv  <- paste0(tmp_folder, module, "-variables-union-mod.csv")

add_novel_vars_to_csv(vars_w_exceptions, union_vars_csv, 
                      out_file = new_vars_csv)
```

Get list of variables from csv file that are of type RECORD or mode
REPEATED in the schema. There shouldn't be any left at this point, but
just in case...

```{r}
#TODO This chunk needs to be tested.
vars_to_remove <- get_record_and_repeated_vars(project, table, schema_csv, 
                                              new_vars_csv)
vars_to_remove

# Remove these variables from the new csv file
if ( length(vars_to_remove ) != 0) {
  remove_vars_from_csv(new_vars_csv, vars_to_remove)
}
```

#### 6. Move modified lists.json and variables.csv files to parent directory

```{r}
# Move up-to-date files to parent directory
file.rename(new_vars_csv, old_vars_csv)
file.rename(new_lists_json, old_lists_json)
```

#### 7. Clean up intermediate files from working directory

```{r}
# Delete intermediate files from directory
unlink(tmp_folder, recursive=TRUE)
```

#### 8. Generate query using Warren Lu-style Query Generator

```{r}
working_dir     <- getwd()
query_generator <- paste0(module, "-query_generator.js")
Sys.setenv(MODULE_FOLDER = module_folder,
           QUERY_GENERATOR = query_generator,
           WORKNG_DIR = working_dir)

# Using shell commands, go to the module folder, run the query generator (node.js),
# and return to the working directory.
system("cd ${MODULE_FOLDER} && node ${QUERY_GENERATOR}; cd ${WORKING_DIR}")
```

#### 9. Update scheduled queries in GCP programatically

```{r}
# Note: If you get this error 
#     "Handshake failed with fatal error SSL_ERROR_SSL: 
#      error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED"
#      disconnect from the VPN and try again. It seems like NIH blocks this.
py_func <- 'gcp_query_tools/update_gcp_scheduled_query_b.py'
Sys.setenv(PY_FUNC = py_func, RUN_NOW = TRUE)

project_id_list = c("nih-nci-dceg-connect-dev", 
                    "nih-nci-dceg-connect-stg-5519",
                    "nih-nci-dceg-connect-prod-6d04")

# system("gcloud auth login") # Use this to authenticate if needed
for (project_id in project_id_list){
  Sys.setenv(PROJECT_ID = project_id)
  system('python3 $PY_FUNC $PROJECT_ID $MODULE_FOLDER $RUN_NOW')
}
```
